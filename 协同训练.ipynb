{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhangyh4\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import time\n",
    "from sklearn import svm\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals import joblib\n",
    "from numpy import * # 要用到delete操作\n",
    "import numpy as np\n",
    "from collections import defaultdict,Counter\n",
    "import math\n",
    "from gensim import corpora,models\n",
    "from scipy.sparse import csr_matrix\n",
    "import pickle as pkl\n",
    "# 不打印警告信息\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('xu_labeled.txt',names=['lines'],sep='\\t',encoding='utf-8')\n",
    "data1 = pd.read_csv('wuxu_labeled.txt',names=['lines'],sep='\\t',encoding='utf-8')\n",
    "frames=[data,data1]\n",
    "unlabeled_data = pd.concat(frames,axis=0)\n",
    "unlabeled_data['label'] = -1\n",
    "labeled_data = pd.read_csv('labeled_data.txt',names=['label','lines'],sep='\\t',encoding='utf-8')\n",
    "frame = [labeled_data,unlabeled_data]\n",
    "sum_data = pd.concat(frame,axis = 0)\n",
    "sum_data = sum_data.dropna()\n",
    "sum_data = sum_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉文本中的空格\n",
    "def process(our_data):\n",
    "    m1 = map(lambda s: s.replace(' ', ''), our_data)\n",
    "    return list(m1)\n",
    "\n",
    "\n",
    "# 让文本只保留汉字\n",
    "def is_chinese(uchar):\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def format_str(content):\n",
    "    content_str = ''\n",
    "    for i in content:\n",
    "        if is_chinese(i):\n",
    "            content_str = content_str + ｉ\n",
    "    return content_str\n",
    "\n",
    "\n",
    "# 对文本进行jieba分词\n",
    "def fenci(datas):\n",
    "    cut_words = map(lambda s: list(jieba.cut(s)), datas)\n",
    "    return list(cut_words)\n",
    "\n",
    "\n",
    "# 去掉文本中的停用词\n",
    "def drop_stopwords(contents, stopwords):\n",
    "    contents_clean = []\n",
    "    for line in contents:\n",
    "        line_clean = []\n",
    "        for word in line:\n",
    "            if word in stopwords:\n",
    "                continue\n",
    "            line_clean.append(word)\n",
    "        contents_clean.append(line_clean)\n",
    "    return contents_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zhangyh4\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.731 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "# 读取停用词表\n",
    "stopwords = pd.read_csv('stopwords.txt', index_col=False, sep=\"\\n\", quoting=3, names=['stopword'], encoding='utf-8')\n",
    "# 对数据进行预处理\n",
    "train_data = process(sum_data.lines.values)\n",
    "\n",
    "chinese_list = []\n",
    "for line in train_data:\n",
    "    chinese_list.append(format_str(line))\n",
    "\n",
    "df_content = pd.DataFrame({'content_S': chinese_list, 'label': sum_data['label']})\n",
    "content_s = fenci(df_content.content_S.values)\n",
    "data_content = pd.DataFrame({'content': content_s})\n",
    "\n",
    "contents = data_content.content.values.tolist()\n",
    "stopwords = stopwords.stopword.values.tolist()\n",
    "contents_clean = drop_stopwords(contents, stopwords)\n",
    "\n",
    "df_data = pd.DataFrame({'contents_clean': contents_clean, 'label': sum_data[\"label\"]})\n",
    "word_list = list(df_data.contents_clean.values)\n",
    "\n",
    "# 将文本处理成tfidf可训练的格式\n",
    "words = []\n",
    "for line_index in range(len(word_list)):\n",
    "    words.append(' '.join(word_list[line_index]))\n",
    "\n",
    "word_list = []\n",
    "for i in range(len(words)):\n",
    "    word_list.append(words[i].split(' '))\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "new_corpus = [dictionary.doc2bow(text) for text in word_list]\n",
    "tfidf = models.TfidfModel(new_corpus)\n",
    "\n",
    "tfidf_vec = []\n",
    "for i in range(len(words)):\n",
    "    string = words[i]\n",
    "    string_bow = dictionary.doc2bow(string.split())\n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    tfidf_vec.append(string_tfidf)\n",
    "\n",
    "lsi_model = models.LsiModel(corpus = tfidf_vec,id2word = dictionary,num_topics=30)\n",
    "\n",
    "lsi_vec = []\n",
    "for i in range(len(words)):\n",
    "    string = words[i]\n",
    "    string_bow = dictionary.doc2bow(string.split())\n",
    "    string_lsi = lsi_model[string_bow]\n",
    "    lsi_vec.append(string_lsi)\n",
    "\n",
    "\n",
    "data = []\n",
    "rows = []\n",
    "cols = []\n",
    "line_count = 0\n",
    "for line in lsi_vec:\n",
    "    for elem in line:\n",
    "        rows.append(line_count)\n",
    "        cols.append(elem[0])\n",
    "        data.append(elem[1])\n",
    "    line_count += 1\n",
    "lsi_sparse_matrix = csr_matrix((data,(rows,cols))) # 稀疏向量\n",
    "lsi_matrix = lsi_sparse_matrix.toarray() # 密集向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(lsi_matrix[:29748]) # 已标注数据\n",
    "x_test = list(lsi_matrix[29748:]) # 未标注数据\n",
    "x_test_ = x_test\n",
    "y_train = list(sum_data[:29748].label.values) # 已标注数据标签\n",
    "y_test = list(sum_data[29748:].label.values)\n",
    "x_test_content = list(sum_data[29748:].lines.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = x_train[:9916]\n",
    "y_train1 = y_train[:9916]\n",
    "\n",
    "x_train2 = x_train[9917:19832]\n",
    "y_train2 = y_train[9917:19832]\n",
    "\n",
    "x_train3 = x_train[19833:]\n",
    "y_train3 = y_train[19833:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train_model(x,y,choose_model):\n",
    "    if choose_model == 'svm':\n",
    "        clf = svm.SVC(probability=True)\n",
    "        clf.fit(x,y)\n",
    "        mdhms = time.strftime('%d%H%M', time.localtime(time.time()))\n",
    "        file = r'C:\\Users\\zhangyh4\\Desktop\\xietong1012\\model\\svm.joblib' + '_' + mdhms\n",
    "        joblib.dump(clf,file)\n",
    "        svm_model = joblib.load(file)\n",
    "        return svm_model\n",
    "    elif choose_model == 'xgboost':\n",
    "        xgbc = XGBClassifier()\n",
    "        xgbc.fit(x,y)\n",
    "        mdhms = time.strftime('%d%H%M', time.localtime(time.time()))\n",
    "        file = r'C:\\Users\\zhangyh4\\Desktop\\xietong1012\\model\\xgboost.joblib' + '_' + mdhms\n",
    "        joblib.dump(xgbc,file)\n",
    "        xgboost_model = joblib.load(file)\n",
    "        return xgboost_model\n",
    "    elif choose_model == 'bayes':\n",
    "        # bayesc = MultinomialNB() 因为向量里不能有负数，一旦有此方法就不行\n",
    "        # bayesc.fit(x,y)\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(x,y)\n",
    "        mdhms = time.strftime('%d%H%M', time.localtime(time.time()))\n",
    "        file = r'C:\\Users\\zhangyh4\\Desktop\\xietong1012\\model\\bayes.joblib' + '_' + mdhms\n",
    "        joblib.dump(gnb,file)\n",
    "        bayes_model = joblib.load(file)\n",
    "        return bayes_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = train_model(np.array(x_train1), y_train1, 'svm')\n",
    "xgboost_model = train_model(np.array(x_train2), y_train2, 'xgboost')\n",
    "bayes_model = train_model(np.array(x_train3), y_train3, 'bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(x_test)):\n",
    "    y_svm_pred = svm_model.predict([x_test[i]])\n",
    "    y_xgboost_pred = xgboost_model.predict([x_test[i]])\n",
    "    y_bayes_pred = bayes_model.predict([x_test[i]])\n",
    "    if y_svm_pred[0] == y_xgboost_pred[0] and y_svm_pred[0] == y_bayes_pred[0]:\n",
    "        x_train.append(x_test[i])\n",
    "        y_train.append(y_svm_pred[0])\n",
    "\n",
    "    elif y_svm_pred[0] == y_xgboost_pred[0] and y_svm_pred[0] != y_bayes_pred[0]:\n",
    "        x_train3.append(x_test[i])\n",
    "        y_train3.append(y_svm_pred[0])\n",
    "        bayes_model = train_model(np.array(x_train3), y_train3, 'bayes')\n",
    "        y_bayes_pred = bayes_model.predict([x_test[i]])\n",
    "        x_train.append(x_test[i])\n",
    "        y_train.append(y_svm_pred[0])\n",
    "\n",
    "    elif y_svm_pred[0] == y_bayes_pred[0] and y_svm_pred[0] != y_xgboost_pred[0]:\n",
    "        x_train2.append(x_test[i])\n",
    "        y_train2.append(y_svm_pred[0])\n",
    "        xgboost_model = train_model(np.array(x_train2), y_train2, 'xgboost')\n",
    "        y_xgboost_pred = xgboost_model.predict([x_test[i]])\n",
    "        x_train.append(x_test[i])\n",
    "        y_train.append(y_svm_pred[0])\n",
    "\n",
    "    elif y_xgboost_pred[0] == y_bayes_pred[0] and y_svm_pred[0] != y_xgboost_pred[0]:\n",
    "        x_train1.append(x_test[i])\n",
    "        y_train1.append(y_svm_pred[0])\n",
    "        svm_model = train_model(np.array(x_train1), y_train1, 'svm')\n",
    "        y_svm_pred = svm_model.predict([x_test[i]])\n",
    "        x_train.append(x_test[i])\n",
    "        y_train.append(y_svm_pred[0]) \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
